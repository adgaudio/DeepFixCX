{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:QualDR dataset labels unavailable because pyjq not installed\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Boilerplate to implement training different networks on different datasets\n",
    "with varying config.\n",
    "\n",
    "I wish a machine could automate setting up decent baseline models and datasets\n",
    "\"\"\"\n",
    "#  import json\n",
    "import os\n",
    "from os.path import exists\n",
    "import pampy\n",
    "from simple_parsing import ArgumentParser, choice\n",
    "from simplepytorch import datasets as D\n",
    "from simplepytorch import trainlib as TL\n",
    "from simplepytorch import metrics\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Union, Optional\n",
    "import dataclasses as dc\n",
    "import numpy as np\n",
    "import torch as T\n",
    "import torchvision.transforms as tvt\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torchvision import models, transforms\n",
    "\n",
    "from deepfix.models import get_effnetv2, get_resnet, get_efficientnetv1, get_DeepFixEnd2End, DeepFixMLP\n",
    "from deepfix.models.ghaarconv import convert_conv2d_to_gHaarConv2d\n",
    "from deepfix.init_from_distribution import init_from_beta, reset_optimizer\n",
    "from deepfix import deepfix_strategies as dfs\n",
    "import pytorch_wavelets as pyw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "# from ipynb.fs.defs.q2 import *\n",
    "import sklearn\n",
    "import torch\n",
    "import torchvision.datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "# YOUR CODE HERE\n",
    "# Use GPU if available, otherwise stick with cpu\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(123)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"device = {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ln -sr /ocean/projects/cie160013p/agaudio/data/CheXpert-v1.0-small ~/store/DeepFix/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "    ('effnetv2', str, str, str): (\n",
    "        lambda pretrain, in_ch, out_ch: get_effnetv2(pretrain, int(in_ch), int(out_ch))),\n",
    "    ('resnet50', str, str, str): (\n",
    "        lambda pretrain, in_ch, out_ch: get_resnet('resnet50', pretrain, int(in_ch), int(out_ch))),\n",
    "    ('resnet18', str, str, str): (\n",
    "        lambda pretrain, in_ch, out_ch: get_resnet('resnet18', pretrain, int(in_ch), int(out_ch))),\n",
    "    ('efficientnet-b0', str, str, str): (\n",
    "        lambda pretrain, in_ch, out_ch: get_efficientnetv1('efficientnet-b0', pretrain, int(in_ch), int(out_ch))),\n",
    "    ('efficientnet-b1', str, str, str): (\n",
    "        lambda pretrain, in_ch, out_ch: get_efficientnetv1('efficientnet-b1', pretrain, int(in_ch), int(out_ch))),\n",
    "    ('waveletres18', str, str, str): lambda pretrain, in_ch, out_ch: R(\n",
    "        pretrain, int(in_ch), int(out_ch)),\n",
    "    ('waveletmlp', str, str, str, str, str, str, str): (\n",
    "        lambda mlp_channels, in_ch, out_ch, wavelet_levels, patch_size, in_ch_mul, mlp_depth: get_DeepFixEnd2End(\n",
    "            int(in_ch), int(out_ch),\n",
    "            in_ch_multiplier=int(in_ch_mul), wavelet='db1',\n",
    "            wavelet_levels=int(wavelet_levels), wavelet_patch_size=int(patch_size),\n",
    "            mlp_depth=int(mlp_depth), mlp_channels=int(mlp_channels),\n",
    "            mlp_fix_weights='none', mlp_activation=None)\n",
    "        ),\n",
    "\n",
    "    #  ('waveletres18v2', str, str, str): lambda pretrain, in_ch, out_ch: (\n",
    "        #  DeepFixCompression(levels=8, wavelet='coif1', patch_size=1),\n",
    "        #  R2(pretrain, int(in_ch), int(out_ch))),\n",
    "}\n",
    "\n",
    "\n",
    "class R(T.nn.Module):\n",
    "    def __init__(self, pretrain, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.r = get_resnet('resnet18', pretrain, in_ch, out_ch,)\n",
    "        self.dwt = pyw.DWT(J=8, wave='coif1', mode='zero')\n",
    "\n",
    "    @staticmethod\n",
    "    def wavelet_coefficients_as_tensorimage(approx, detail, normalize=False):\n",
    "        B,C = approx.shape[:2]\n",
    "        fixed_dims = approx.shape[:-2] # num images in minibatch, num channels, etc\n",
    "        output_shape = fixed_dims + (\n",
    "            detail[0].shape[-2]*2,  # input img height\n",
    "            detail[0].shape[-1]*2)  # input img width\n",
    "        im = T.zeros(output_shape, device=approx.device, dtype=approx.dtype)\n",
    "        if normalize:\n",
    "            norm11 = lambda x: (x / max(x.min()*-1, x.max()))  # into [-1,+1] preserving sign\n",
    "            #  approx = norm11(approx)\n",
    "        im[..., :approx.shape[-2], :approx.shape[-1]] = approx if approx is not None else 0\n",
    "        for level in detail:\n",
    "            lh, hl, hh = level.unbind(-3)\n",
    "            h,w = lh.shape[-2:]\n",
    "            if normalize:\n",
    "                lh, hl, hh = [norm11(x) for x in [lh, hl, hh]]\n",
    "            #  im[:h, :w] = approx\n",
    "            im[..., 0:h, w:w+w] = lh  # horizontal\n",
    "            im[..., h:h+h, :w] = hl  # vertical\n",
    "            im[..., h:h+h, w:w+w] = hh  # diagonal\n",
    "        return im\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.wavelet_coefficients_as_tensorimage(*self.dwt(x))\n",
    "        return self.r(x)\n",
    "\n",
    "\n",
    "class R2(T.nn.Module):\n",
    "    def __init__(self, pretrain, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.r = get_resnet('resnet18', pretrain, in_ch, out_ch,)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,C,H = x.shape\n",
    "        x = x.unsqueeze(-1).repeat(1,1,1,H)\n",
    "        return self.r(x)\n",
    "\n",
    "\n",
    "class LossCheXpertIdentity(T.nn.Module):\n",
    "    def __init__(self, N):\n",
    "        super().__init__()\n",
    "        self.bce = T.nn.BCEWithLogitsLoss()\n",
    "        self.N = N\n",
    "\n",
    "    def forward(self, yhat, y):\n",
    "        # absolute max possible num patients in chexpert is 223414\n",
    "        # but let's just hash them into a smaller number of bins via modulo N\n",
    "        assert self.N == yhat.shape[1], \\\n",
    "                f'note: model must have {self.N} binary predictions per sample'\n",
    "        y_onehot = y.new_zeros(y.shape[0], self.N, dtype=T.float\n",
    "                               ).scatter_(1, y.long()%self.N, 1)\n",
    "        return self.bce(yhat[:, -1], y_onehot[:, -1])\n",
    "\n",
    "\n",
    "class LossCheXpertUignore(T.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bce = T.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, yhat, y):\n",
    "        ignore = (y != 2)  # ignore uncertainty labels\n",
    "        return self.bce(yhat[ignore], y[ignore])\n",
    "\n",
    "\n",
    "def loss_intelmobileodt(yhat, y):\n",
    "    \"\"\"BCE Loss with class balancing weights.\n",
    "\n",
    "    Not sure this actually helps\n",
    "\n",
    "    because Type 2 is the hardest class, it\n",
    "    has the most samples, and it separates Type 1 from Type 3.  Arguably, Type 2\n",
    "    samples are on the decision boundary between Type 1 and 3.\n",
    "    Class balancing weights make it harder to focus on class 2.\n",
    "    \"\"\"\n",
    "    #  assert y.shape == yhat.shape, 'sanity check'\n",
    "    #  assert y.dtype == yhat.dtype, 'sanity check'\n",
    "\n",
    "    # class distribution of stage='train'\n",
    "    w = T.tensor([249, 781, 450], dtype=y.dtype, device=y.device)\n",
    "    w = (w.max() / w).reshape(1, 3)\n",
    "    # w can have any of the shapes:  (B,1) or (1,C) or (B,C)\n",
    "    #  return T.nn.functional.binary_cross_entropy_with_logits(yhat, y, weight=w)\n",
    "    return T.nn.functional.cross_entropy(yhat, y, weight=w)\n",
    "    # can't apply focal loss unless do it manually.\n",
    "\n",
    "\n",
    "def onehot(y, nclasses):\n",
    "    return T.zeros((y.numel(), nclasses), dtype=y.dtype, device=y.device)\\\n",
    "            .scatter_(1, y.unsqueeze(1), 1)\n",
    "\n",
    "\n",
    "def _upsample_pad_minibatch_imgs_to_same_size(batch, target_is_segmentation_mask=False):\n",
    "    \"\"\"a collate function for a dataloader of (x,y) samples.  \"\"\"\n",
    "    shapes = [item[0].shape for item in batch]\n",
    "    H = max(h for c,h,w in shapes)\n",
    "    W = max(w for c,h,w in shapes)\n",
    "    X, Y = [], []\n",
    "    for item in batch:\n",
    "        h,w = item[0].shape[1:]\n",
    "        dh, dw = (H-h), (W-w)\n",
    "        padding = (dw//2, dw-dw//2, dh//2, dh-dh//2, )\n",
    "        X.append(T.nn.functional.pad(item[0], padding))\n",
    "        if target_is_segmentation_mask:\n",
    "            Y.append(T.nn.functional.pad(item[1], padding))\n",
    "        else:\n",
    "            Y.append(item[1])\n",
    "    return T.stack(X), T.stack(Y)\n",
    "\n",
    "\n",
    "def get_dset_chexpert(train_frac=.8, val_frac=.2, small=False,\n",
    "                      labels:str='diagnostic', num_identities=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        labels:  either \"diagnostic\" (the 14 classes defined as\n",
    "            D.CheXpert.LABELS_DIAGNOSTIC) or \"identity\" (\"patient\", \"study\",\n",
    "            \"view\", \"index\")\n",
    "        small:  whether to use CheXpert_Small dataset (previously downsampled\n",
    "            images) or the fully size dataset.\n",
    "        num_identities:  used only if labels='identity'.  If\n",
    "            num_identities=1000, then all patients get identified as coming\n",
    "            from precisely 1 of 1000 bins.\n",
    "\n",
    "    Returns:\n",
    "        (\n",
    "        {'train_dset': ..., 'val_dset': ..., 'test_dset': ...,\n",
    "         'train_loader': ..., 'val_loader': ..., 'test_loader': ...\n",
    "         },\n",
    "\n",
    "        ('Pneumonia', 'Cardiomegaly', ...)  # class names defined by `labels`\n",
    "        )\n",
    "    \"\"\"\n",
    "    _label_cleanup_dct = dict(D.CheXpert.LABEL_CLEANUP_DICT)\n",
    "    if labels == 'diagnostic':\n",
    "        class_names = D.CheXpert.LABELS_DIAGNOSTIC\n",
    "        for k in class_names:\n",
    "            _label_cleanup_dct[k][np.nan] = 0  # remap missing value to negative\n",
    "        get_ylabels = lambda dct: \\\n",
    "                D.CheXpert.format_labels(dct, labels=class_names).float()\n",
    "    elif labels == 'identity':\n",
    "        class_names = list(range(num_identities))\n",
    "        get_ylabels = lambda dct: \\\n",
    "                (D.CheXpert.format_labels(dct, labels=['index']) % num_identities).long()\n",
    "    else:\n",
    "        raise NotImplementedError(f\"unrecognized labels: {labels}\")\n",
    "    kws = dict(\n",
    "        img_transform=tvt.Compose([\n",
    "            #  tvt.RandomCrop((512, 512)),\n",
    "            tvt.ToTensor(),  # full res 1024x1024 imgs\n",
    "            tvt.Resize((224, 224)),\n",
    "            tvt.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "            \n",
    "        ]),\n",
    "        getitem_transform=lambda dct: (dct['image'], get_ylabels(dct)),\n",
    "        label_cleanup_dct=_label_cleanup_dct,\n",
    "    )\n",
    "    if small:\n",
    "        kls = D.CheXpert_Small\n",
    "    else:\n",
    "        kls = D.CheXpert\n",
    "\n",
    "    train_dset = kls(dataset_dir=\"../data/CheXpert-v1.0-small/\",use_train_set=True, **kws)  #Edited Elvin\n",
    "\n",
    "    N = len(train_dset)\n",
    "    if train_frac + val_frac == 1:\n",
    "        nsplits = [N - int(N*val_frac), int(N*val_frac), 0]\n",
    "    else:\n",
    "        a,b = int(N*train_frac), int(N*val_frac)\n",
    "        nsplits = [a,b, N-a-b]\n",
    "    train_dset, val_dset, _ = T.utils.data.random_split(train_dset, nsplits)\n",
    "    test_dset = kls(dataset_dir=\"../data/CheXpert-v1.0-small/\",use_train_set=False, **kws) #Edited Elvin\n",
    "    batch_dct = dict(\n",
    "        batch_size=256, collate_fn=_upsample_pad_minibatch_imgs_to_same_size,\n",
    "        num_workers=int(os.environ.get(\"num_workers\", 4)))  # upsample pad must take time\n",
    "    train_loader=DataLoader(train_dset, shuffle=True, **batch_dct)\n",
    "    val_loader=DataLoader(val_dset, **batch_dct)\n",
    "    test_loader=DataLoader(test_dset, **batch_dct)\n",
    "    return (dict(\n",
    "        train_dset=train_dset, val_dset=val_dset, test_dset=test_dset,\n",
    "        train_loader=train_loader, val_loader=val_loader, test_loader=test_loader,\n",
    "    ), class_names)\n",
    "\n",
    "\n",
    "\n",
    "def match(spec:str, dct:dict):\n",
    "    return pampy.match(spec.split(':'), *(x for y in dct.items() for x in y))\n",
    "\n",
    "\n",
    "def get_model_opt_loss(\n",
    "        model_spec:str, opt_spec:str, loss_spec:str, regularizer_spec:str,\n",
    "        device:str) -> dict[str, Union[T.nn.Module, T.optim.Optimizer]]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model_spec: a string of form,\n",
    "            \"model_name:pretraining:in_channels:out_classes\".  For example:\n",
    "            \"effnetv2:untrained:1:5\"\n",
    "        opt_spec: Specifies how to create optimizer.\n",
    "            First value is a pytorch Optimizer in T.optim.*.\n",
    "            Other values are numerical parameters.\n",
    "            Example: \"SGD:lr=.003:momentum=.9\"\n",
    "        device: e.g. 'cpu' or 'gpu'\n",
    "    Returns:\n",
    "        a pytorch model and optimizer\n",
    "    \"\"\"\n",
    "    mdl = match(model_spec, MODELS)\n",
    "    mdl = mdl.to(device, non_blocking=True)\n",
    "    optimizer = reset_optimizer(opt_spec, mdl)\n",
    "    loss_fn = match(loss_spec, LOSS_FNS)\n",
    "    if regularizer_spec != 'none':\n",
    "        loss_fn = RegularizedLoss(mdl, loss_fn, regularizer_spec)\n",
    "    return dict(model=mdl, optimizer=optimizer, loss_fn=loss_fn)\n",
    "\n",
    "\n",
    "class RegularizedLoss(T.nn.Module):\n",
    "    def __init__(self, model, lossfn, regularizer_spec:str):\n",
    "        super().__init__()\n",
    "        self.lossfn = lossfn\n",
    "        self.regularizer_spec = regularizer_spec\n",
    "        if regularizer_spec == 'none':\n",
    "            self.regularizer = lambda *y: 0\n",
    "        elif regularizer_spec.startswith('deepfixmlp:'):\n",
    "            lbda = float(regularizer_spec.split(':')[1])\n",
    "            self.regularizer = lambda *y: (\n",
    "                float(lbda) * DeepFixMLP.get_VecAttn_regularizer(model))\n",
    "        else:\n",
    "            raise NotImplementedError(regularizer_spec)\n",
    "\n",
    "    def forward(self, yhat, y):\n",
    "        return self.lossfn(yhat, y) + self.regularizer(yhat, y)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'RegularizedLoss<{repr(self.lossfn)},{self.regularizer_spec}>'\n",
    "\n",
    "\n",
    "def get_dset_loaders_resultfactory(dset_spec:str) -> dict:\n",
    "    dct, class_names = match(dset_spec, DSETS)\n",
    "    if any(dset_spec.startswith(x) for x in {'intel_mobileodt:',\n",
    "                                             'chexpert_small_ID:'}):\n",
    "        #  dct['result_factory'] = lambda: TL.MultiLabelBinaryClassification(\n",
    "                #  class_names, binarize_fn=lambda yh: (T.sigmoid(yh)>.5).long())\n",
    "        dct['result_factory'] = lambda: TL.MultiClassClassification(\n",
    "                len(class_names), binarize_fn=lambda yh: yh.softmax(1).argmax(1))\n",
    "    elif any(dset_spec.startswith(x) for x in {'chexpert:', 'chexpert_small:'}):\n",
    "        dct['result_factory'] = lambda: CheXpertMultiLabelBinaryClassification(\n",
    "            class_names, binarize_fn=lambda yh: (yh.sigmoid()>.5).long(), report_avg=True)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"I don't know how to create the result factory for {dset_spec}\")\n",
    "    return dct\n",
    "\n",
    "class CheXpertMultiLabelBinaryClassification(TL.MultiLabelBinaryClassification):\n",
    "    def update(self, yhat, y, loss) -> None:\n",
    "        self.num_samples += yhat.shape[0]\n",
    "        self.loss += loss.item()\n",
    "        assert yhat.shape == y.shape\n",
    "        assert yhat.ndim == 2 and yhat.shape[1] == len(self._cms), \"sanity check: model outputs expected prediction shape\"\n",
    "        binarized = self._binarize_fn(yhat)\n",
    "        assert binarized.dtype == T.long, 'sanity check binarize fn'\n",
    "        assert binarized.shape == y.shape, 'sanity check binarize fn'\n",
    "        ignore = (y != 2)  # ignore uncertainty labels\n",
    "        for i, (kls, cm) in enumerate(self._cms.items()):\n",
    "            rows = ignore[:, i]\n",
    "            if rows.sum() == 0:\n",
    "                continue  # don't update a confusion matrix if all data for this class is ignored\n",
    "            cm += metrics.confusion_matrix(y[rows, i], binarized[rows, i], num_classes=2).cpu()\n",
    "\n",
    "\n",
    "def get_deepfix_train_strategy(args:'TrainOptions'):\n",
    "    deepfix_spec = args.deepfix\n",
    "    if deepfix_spec == 'off':\n",
    "        return TL.train_one_epoch\n",
    "    elif deepfix_spec.startswith('reinit:'):\n",
    "        _, N, P, R = deepfix_spec.split(':')\n",
    "        return dfs.DeepFix_TrainOneEpoch(int(N), float(P), int(R), TL.train_one_epoch)\n",
    "    elif deepfix_spec.startswith('dhist:'):\n",
    "        fp = deepfix_spec.split(':', 1)[1]\n",
    "        assert exists(fp), f'histogram file not found: {fp}'\n",
    "        return dfs.DeepFix_DHist(fp)\n",
    "    elif deepfix_spec.startswith('dfhist:'):\n",
    "        fp = deepfix_spec.split(':', 1)[1]\n",
    "        assert exists(fp), f'histogram file not found: {fp}'\n",
    "        return dfs.DeepFix_DHist(fp, fixed=True)\n",
    "    elif deepfix_spec == 'fixed':\n",
    "        return dfs.DeepFix_DHist('', fixed=True, init_with_hist=False)\n",
    "    elif deepfix_spec.startswith('beta:'):\n",
    "        alpha, beta = deepfix_spec.split(':')[1:]\n",
    "        return dfs.DeepFix_LambdaInit(\n",
    "            lambda cfg: init_from_beta(cfg.model, float(alpha), float(beta)))\n",
    "    elif deepfix_spec.startswith('ghaarconv2d:'):\n",
    "        ignore_layers = deepfix_spec.split(':')[1].split(',')\n",
    "        return dfs.DeepFix_LambdaInit(\n",
    "            lambda cfg: (\n",
    "                print(f'initialize {deepfix_spec}'),\n",
    "                convert_conv2d_to_gHaarConv2d(cfg.model, ignore_layers=ignore_layers),\n",
    "                reset_optimizer(args.opt, cfg.model),\n",
    "                print(cfg.model)\n",
    "            ))\n",
    "    else:\n",
    "        raise NotImplementedError(deepfix_spec)\n",
    "\n",
    "\n",
    "def train_config(args:'TrainOptions') -> TL.TrainConfig:\n",
    "    return TL.TrainConfig(\n",
    "        **get_model_opt_loss(\n",
    "            args.model, args.opt, args.lossfn, args.loss_reg, args.device),\n",
    "        **get_dset_loaders_resultfactory(args.dset),\n",
    "        device=args.device,\n",
    "        epochs=args.epochs,\n",
    "        start_epoch=args.start_epoch,\n",
    "        train_one_epoch=get_deepfix_train_strategy(args),\n",
    "        experiment_id=args.experiment_id,\n",
    "    )\n",
    "\n",
    "\n",
    "@dc.dataclass\n",
    "class TrainOptions:\n",
    "    \"\"\"High-level configuration for training PyTorch models\n",
    "    on the IntelMobileODTCervical dataset.\n",
    "    \"\"\"\n",
    "    epochs:int = 50\n",
    "    start_epoch:int = 0  # if \"--start_epoch 1\", then don't evaluate perf before training.\n",
    "    device:str = 'cuda' if T.cuda.is_available() else 'cpu'\n",
    "    dset:str = None #choice(\n",
    "        #  'intel_mobileodt:train:val:test:v1',\n",
    "        #  'intel_mobileodt:train+additional:val:test:v1',\n",
    "        #  'intel_mobileodt:train+additional:noval:test:v1',\n",
    "        #  'chexpert:.8:.2', 'chexpert:.01:.01', 'chexpert:.001:.001',\n",
    "        #  'chexpert_small:.8:.2', 'chexpert_small:.01:.01',\n",
    "        #   'chexpert_small:.001:.001',\n",
    "        #  default='intel_mobileodt:train:val:test:v1')\n",
    "    opt:str = 'SGD:lr=.001:momentum=.9:nesterov=1'\n",
    "    lossfn:str = None  # choices:\n",
    "        #  'BCEWithLogitsLoss',\n",
    "        #  'CrossEntropyLoss', \n",
    "        #  'CE_intelmobileodt',\n",
    "        #  'chexpert_uignore', \n",
    "        #  'chexpert_identity:N' for some N=num_identities predicted by model (compared to identities y%N)\n",
    "    loss_reg:str = 'none'  # Optionally add a regularizer to the loss.  loss + reg.  Accepted values:  'none', 'deepfixmlp:X' where X is a positive float denoting the lambda in l1 regularizer\n",
    "    model:str = 'resnet18:imagenet:3:3'  # Model specification adheres to the template \"model_name:pretraining:in_ch:out_ch\"\n",
    "    deepfix:str = 'off'  # DeepFix Re-initialization Method.\n",
    "                         #  \"off\" or \"reinit:N:P:R\" or \"d[f]hist:path_to_histogram.pth\"\n",
    "                         #  or \"beta:A:B\" for A,B as (float) parameters of the beta distribution\n",
    "                         # 'ghaarconv2d:layer1,layer2' Replaces all spatial convolutions with GHaarConv2d layer except the specified layers\n",
    "    experiment_id:str = os.environ.get('run_id', 'debugging')\n",
    "    prune:str = 'off'\n",
    "\n",
    "    def execute(self):\n",
    "        cfg = train_config(self)\n",
    "        cfg.train(cfg)\n",
    "\n",
    "\n",
    "def main():\n",
    "    p = ArgumentParser()\n",
    "    p.add_arguments(TrainOptions, dest='TrainOptions')\n",
    "#     for patch_size in [1,32]:\n",
    "#         for wavelet_level in [1,2,3,4,5,6,7,8,9]:    \n",
    "#             try:\n",
    "    in_ch, out_ch = 3, 3\n",
    "    model_params = \"resnet18:imagenet:\"+str(in_ch)+\":\"+str(in_ch)    \n",
    "    \n",
    "#     model_params = \"waveletmlp:300:1:14:\"+str(patch_size)+\":\"+str(wavelet_level)+\":1:2\"\n",
    "    exp_id = 'model_'+model_params+'_in_ch_'+str(in_ch)+'out_ch_'+str(in_ch)#+'_patch_size_' + str(patch_size) + '_level_' + str(wavelet_level)\n",
    "    args = p.parse_args([\"--dset\", \"chexpert_small:.01:.01\", \"--opt\", \"Adam:lr=0.001\", \"--lossfn\", \"chexpert_uignore\", \"--model\", model_params, \"--loss_reg\", \"none\",\"--experiment_id\",exp_id]).TrainOptions\n",
    "\n",
    "    print(args)\n",
    "    cfg = train_config(args)\n",
    "\n",
    "# python deepfix/train.py --dset chexpert_small:.01:.01 --opt Adam:lr=0.001 --lossfn chexpert_uignore --model waveletmlp:300:1:14:7:1:1:2 --loss_reg none    \n",
    "\n",
    "    if args.prune != 'off':\n",
    "        assert args.prune.startswith('ChannelPrune:')\n",
    "        raise NotImplementedError('code is a bit hardcoded, so it is not available without hacking on it.')\n",
    "        print(args.prune)\n",
    "        from explainfix import channelprune\n",
    "        from deepfix.weight_saliency import costfn_multiclass\n",
    "        a = sum([x.numel() for x in cfg.model.parameters()])\n",
    "        channelprune(cfg.model, pct=5, grad_cost_fn=costfn_multiclass,\n",
    "                     loader=cfg.train_loader, device=cfg.device, num_minibatches=10)\n",
    "        b = sum([x.numel() for x in cfg.model.parameters()])\n",
    "        assert a/b != 1\n",
    "        print(f'done channelpruning.  {a/b}')\n",
    "\n",
    "    cfg.train(cfg)\n",
    "#             except Exception as e:\n",
    "#                 print(\"=================================================================================================\")\n",
    "#                 print(e)\n",
    "#                 print(\"=================================================================================================\")\n",
    "            \n",
    "    print('+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "    #  import IPython ; IPython.embed() ; import sys ; sys.exit()\n",
    "\n",
    "    #  with T.profiler.profile(\n",
    "    #      activities=[\n",
    "    #          T.profiler.ProfilerActivity.CPU,\n",
    "    #          T.profiler.ProfilerActivity.CUDA,\n",
    "    #      ], with_modules=True,\n",
    "    #  ) as p:\n",
    "    #      cfg.train(cfg)\n",
    "    #  print(p.key_averages().table(\n",
    "    #      sort_by=\"self_cuda_time_total\", row_limit=-1))\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Dataloader for a batch size 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_dset': <torch.utils.data.dataset.Subset at 0x14aa53dfb550>,\n",
       " 'val_dset': <torch.utils.data.dataset.Subset at 0x14aa53dfb580>,\n",
       " 'test_dset': <simplepytorch.datasets.chexpert.CheXpert at 0x14aa53dff7c0>,\n",
       " 'train_loader': <torch.utils.data.dataloader.DataLoader at 0x14aa53dffe50>,\n",
       " 'val_loader': <torch.utils.data.dataloader.DataLoader at 0x14aa53dfbd00>,\n",
       " 'test_loader': <torch.utils.data.dataloader.DataLoader at 0x14aa53dfb910>}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d,l = get_dset_chexpert()\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune the Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "# for param in model.parameters():\n",
    "#     print(param)\n",
    "#     param.requires_grad = False\n",
    "for name, param in model.named_parameters():\n",
    "#     print(name)\n",
    "    if not 'layer4' in name:\n",
    "        param.requires_grad = False\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "# model.parameters\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 14)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opposed to before.\n",
    "optimizer_conv = optim.SGD(model.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "#     since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train_loader', 'val_loader']:\n",
    "            if phase == 'train_loader':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in tqdm(d[phase],desc=f\"Epoch {epoch}\"):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train_loader'):\n",
    "                    outputs = model(inputs)\n",
    "                    preds = outputs\n",
    "#                     print(outputs.shape)\n",
    "#                     _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train_loader':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train_loader':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / (len(d[phase])*batch_size)\n",
    "            epoch_acc = running_corrects.double() / (len(d[phase])*batch_size)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val_loader' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "#     time_elapsed = time.time() - since\n",
    "#     print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "#         time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 699/699 [25:08<00:00,  2.16s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loader Loss: 0.5935 Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 175/175 [05:16<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loader Loss: 0.5287 Acc: 0.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 699/699 [23:16<00:00,  2.00s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loader Loss: 0.5134 Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 175/175 [05:11<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loader Loss: 0.4987 Acc: 0.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 699/699 [24:25<00:00,  2.10s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loader Loss: 0.4937 Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 175/175 [05:45<00:00,  1.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loader Loss: 0.4861 Acc: 0.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 699/699 [23:15<00:00,  2.00s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loader Loss: 0.4847 Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 175/175 [06:44<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loader Loss: 0.4799 Acc: 0.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 699/699 [22:13<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loader Loss: 0.4795 Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 175/175 [05:17<00:00,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loader Loss: 0.4763 Acc: 0.0000\n",
      "\n",
      "Best val Acc: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, criterion, optimizer_conv,\n",
    "                         exp_lr_scheduler, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the last layer of the Resnet\n",
    "new_model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "new_model = new_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Dataloader again, on a Batch size of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_dset_chexpert(train_frac=.8, val_frac=.2, small=False,\n",
    "                      labels:str='diagnostic', num_identities=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        labels:  either \"diagnostic\" (the 14 classes defined as\n",
    "            D.CheXpert.LABELS_DIAGNOSTIC) or \"identity\" (\"patient\", \"study\",\n",
    "            \"view\", \"index\")\n",
    "        small:  whether to use CheXpert_Small dataset (previously downsampled\n",
    "            images) or the fully size dataset.\n",
    "        num_identities:  used only if labels='identity'.  If\n",
    "            num_identities=1000, then all patients get identified as coming\n",
    "            from precisely 1 of 1000 bins.\n",
    "\n",
    "    Returns:\n",
    "        (\n",
    "        {'train_dset': ..., 'val_dset': ..., 'test_dset': ...,\n",
    "         'train_loader': ..., 'val_loader': ..., 'test_loader': ...\n",
    "         },\n",
    "\n",
    "        ('Pneumonia', 'Cardiomegaly', ...)  # class names defined by `labels`\n",
    "        )\n",
    "    \"\"\"\n",
    "    _label_cleanup_dct = dict(D.CheXpert.LABEL_CLEANUP_DICT)\n",
    "    if labels == 'diagnostic':\n",
    "        class_names = D.CheXpert.LABELS_DIAGNOSTIC\n",
    "        for k in class_names:\n",
    "            _label_cleanup_dct[k][np.nan] = 0  # remap missing value to negative\n",
    "        get_ylabels = lambda dct: \\\n",
    "                D.CheXpert.format_labels(dct, labels=class_names).float()\n",
    "    elif labels == 'identity':\n",
    "        class_names = list(range(num_identities))\n",
    "        get_ylabels = lambda dct: \\\n",
    "                (D.CheXpert.format_labels(dct, labels=['index']) % num_identities).long()\n",
    "    else:\n",
    "        raise NotImplementedError(f\"unrecognized labels: {labels}\")\n",
    "    kws = dict(\n",
    "        img_transform=tvt.Compose([\n",
    "            #  tvt.RandomCrop((512, 512)),\n",
    "            tvt.ToTensor(),  # full res 1024x1024 imgs\n",
    "            tvt.Resize((224, 224)),\n",
    "            tvt.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "            \n",
    "        ]),\n",
    "        getitem_transform=lambda dct: (dct['image'], get_ylabels(dct)),\n",
    "        label_cleanup_dct=_label_cleanup_dct,\n",
    "    )\n",
    "    if small:\n",
    "        kls = D.CheXpert_Small\n",
    "    else:\n",
    "        kls = D.CheXpert\n",
    "\n",
    "    train_dset = kls(dataset_dir=\"../data/CheXpert-v1.0-small/\",use_train_set=True, **kws)  #Edited Elvin\n",
    "\n",
    "    N = len(train_dset)\n",
    "    if train_frac + val_frac == 1:\n",
    "        nsplits = [N - int(N*val_frac), int(N*val_frac), 0]\n",
    "    else:\n",
    "        a,b = int(N*train_frac), int(N*val_frac)\n",
    "        nsplits = [a,b, N-a-b]\n",
    "    train_dset, val_dset, _ = T.utils.data.random_split(train_dset, nsplits)\n",
    "    test_dset = kls(dataset_dir=\"../data/CheXpert-v1.0-small/\",use_train_set=False, **kws) #Edited Elvin\n",
    "    batch_dct = dict(\n",
    "        batch_size=1, collate_fn=_upsample_pad_minibatch_imgs_to_same_size,\n",
    "        num_workers=int(os.environ.get(\"num_workers\", 4)))  # upsample pad must take time\n",
    "    train_loader=DataLoader(train_dset, shuffle=True, **batch_dct)\n",
    "    val_loader=DataLoader(val_dset, **batch_dct)\n",
    "    test_loader=DataLoader(test_dset, **batch_dct)\n",
    "    return (dict(\n",
    "        train_dset=train_dset, val_dset=val_dset, test_dset=test_dset,\n",
    "        train_loader=train_loader, val_loader=val_loader, test_loader=test_loader,\n",
    "    ), class_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_dset': <torch.utils.data.dataset.Subset at 0x14aa53dfb850>,\n",
       " 'val_dset': <torch.utils.data.dataset.Subset at 0x14aa53dfba00>,\n",
       " 'test_dset': <simplepytorch.datasets.chexpert.CheXpert at 0x14aa5a14dee0>,\n",
       " 'train_loader': <torch.utils.data.dataloader.DataLoader at 0x14aa5ac9b490>,\n",
       " 'val_loader': <torch.utils.data.dataloader.DataLoader at 0x14aa53dfffa0>,\n",
       " 'test_loader': <torch.utils.data.dataloader.DataLoader at 0x14aa310adf40>}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d,l = get_dset_chexpert()\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_loader = d['train_loader']\n",
    "valset_loader = d['val_loader']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 178732/178732 [33:43<00:00, 88.34it/s]  \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Will contain the feature\n",
    "X_train = []\n",
    "Y_train = []\n",
    "for data in tqdm(trainset_loader):\n",
    "    inputs, labels = data\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    with torch.no_grad():\n",
    "        # Extract the feature from the image\n",
    "        feature = new_model(inputs)\n",
    "        X_train.append(feature.cpu().detach().numpy().reshape(512))\n",
    "        Y_train.append(labels.cpu().detach().numpy()[0])\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44682/44682 [08:46<00:00, 84.93it/s]  \n"
     ]
    }
   ],
   "source": [
    "# Will contain the feature\n",
    "X_test = []\n",
    "Y_test = []\n",
    "for data in tqdm(valset_loader):\n",
    "    inputs, labels = data\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    with torch.no_grad():\n",
    "        # Extract the feature from the image\n",
    "        feature = new_model(inputs)\n",
    "#         print(feature.size())\n",
    "        # Convert to NumPy Array, Reshape it, and save it to features variable\n",
    "        X_test.append(feature.cpu().detach().numpy().reshape(512))\n",
    "        Y_test.append(labels.cpu().detach().numpy()[0])\n",
    "#         features.append(feature.cpu().detach().numpy().reshape(-1,4096))\n",
    "# Convert to NumPy Array\n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train).to_csv(\"feature_csv/resnet18_x_train_f.csv\", index=False)  \n",
    "pd.DataFrame(Y_train).to_csv(\"feature_csv/resnet18_y_train_f.csv\", index=False)\n",
    "# X_train = pd.read_csv(\"feature_csv/resnet18_x_train.csv\")\n",
    "# Y_train = pd.read_csv(\"feature_csv/resnet18_y_train.csv\")\n",
    "\n",
    "pd.DataFrame(X_test).to_csv(\"feature_csv/resnet18_x_test_f.csv\", index=False)  \n",
    "pd.DataFrame(Y_test).to_csv(\"feature_csv/resnet18_y_test_f.csv\", index=False)\n",
    "# X_test = pd.read_csv(\"feature_csv/resnet18_x_test.csv\")\n",
    "# Y_test = pd.read_csv(\"feature_csv/resnet18_y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(X_train).to_csv(\"feature_csv/vgg16_x_train.csv\", index=False)  \n",
    "# pd.DataFrame(Y_train).to_csv(\"feature_csv/vgg16_y_train.csv\", index=False)\n",
    "X_train = pd.read_csv(\"feature_csv/resnet18_x_train_f.csv\")\n",
    "Y_train = pd.read_csv(\"feature_csv/resnet18_y_train_f.csv\")\n",
    "\n",
    "# pd.DataFrame(X_test).to_csv(\"feature_csv/vgg16_x_test.csv\", index=False)  \n",
    "# pd.DataFrame(Y_test).to_csv(\"feature_csv/vgg16_y_test.csv\", index=False)\n",
    "X_test = pd.read_csv(\"feature_csv/resnet18_x_test_f.csv\")\n",
    "Y_test = pd.read_csv(\"feature_csv/resnet18_y_test_f.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.iloc[:,:].values\n",
    "Y_train = Y_train.iloc[:,:].values\n",
    "\n",
    "X_test = X_test.iloc[:,:].values\n",
    "Y_test = Y_test.iloc[:,:].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 0 (pos) is trained\n",
      "model 0 (neg) is trained\n",
      "model 1 (pos) is trained\n",
      "model 1 (neg) is trained\n",
      "model 2 (pos) is trained\n",
      "model 2 (neg) is trained\n",
      "model 3 (pos) is trained\n",
      "model 3 (neg) is trained\n",
      "model 4 (pos) is trained\n",
      "model 4 (neg) is trained\n",
      "model 5 (pos) is trained\n",
      "model 5 (neg) is trained\n",
      "model 6 (pos) is trained\n",
      "model 6 (neg) is trained\n",
      "model 7 (pos) is trained\n",
      "model 7 (neg) is trained\n",
      "model 8 (pos) is trained\n",
      "model 8 (neg) is trained\n",
      "model 9 (pos) is trained\n",
      "model 9 (neg) is trained\n",
      "model 10 (pos) is trained\n",
      "model 10 (neg) is trained\n",
      "model 11 (pos) is trained\n",
      "model 11 (neg) is trained\n",
      "model 12 (pos) is trained\n",
      "model 12 (neg) is trained\n",
      "model 13 (pos) is trained\n",
      "model 13 (neg) is trained\n",
      "Trained\n"
     ]
    }
   ],
   "source": [
    "# pos_preds = []\n",
    "# neg_preds = []\n",
    "\n",
    "# pos_preds_probs = []\n",
    "# neg_preds_probs = []\n",
    "\n",
    "pos_scores = []\n",
    "neg_scores = []\n",
    "\n",
    "gmm_pos_models   = []\n",
    "gmm_neg_models   = []\n",
    "\n",
    "y_pred = np.zeros_like(Y_test)\n",
    "for i in range(14):\n",
    "    x_pos_train = X_train[Y_train[:, i] == 1]\n",
    "    x_neg_train = X_train[Y_train[:, i] == 0]\n",
    "    \n",
    "    gmm_pos = GaussianMixture(n_components=3,init_params='random',covariance_type='diag',tol=0.001).fit(x_pos_train)\n",
    "    print(\"model {} (pos) is trained\".format(i) )\n",
    "    \n",
    "    gmm_neg = GaussianMixture(n_components=3,init_params='kmeans',covariance_type='diag',tol=0.001).fit(x_neg_train)\n",
    "    print(\"model {} (neg) is trained\".format(i) )\n",
    "\n",
    "    pos_preds = gmm_pos.score_samples(X_test)\n",
    "    neg_preds = gmm_neg.score_samples(X_test)\n",
    "    \n",
    "    for j in range(pos_preds.shape[0]):\n",
    "        if pos_preds[j]>neg_preds[j]:\n",
    "            y_pred[j,i] = 1\n",
    "\n",
    "    gmm_pos_models.append(gmm_pos)\n",
    "    gmm_neg_models.append(gmm_neg)\n",
    "\n",
    "#     print(\"model {} is trained\".format(i) )\n",
    "print('Trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class 0: 0.06705160914909807\n",
      "Accuracy for class 1: 0.08054697641108276\n",
      "Accuracy for class 2: 0.1092833803321248\n",
      "Accuracy for class 3: 0.2998522895125554\n",
      "Accuracy for class 4: 0.032429166107157244\n",
      "Accuracy for class 5: 0.2383510138310729\n",
      "Accuracy for class 6: 0.1674947406114319\n",
      "Accuracy for class 7: 0.09397520254241082\n",
      "Accuracy for class 8: 0.2748310281545141\n",
      "Accuracy for class 9: 0.06984915625979142\n",
      "Accuracy for class 10: 0.29372006624591557\n",
      "Accuracy for class 11: 0.02468555570475807\n",
      "Accuracy for class 12: 0.027595004699879147\n",
      "Accuracy for class 13: 0.34087552034376256\n"
     ]
    }
   ],
   "source": [
    "# 3 componenets, Spherical Covariance\n",
    "for i in range(Y_test.shape[1]):\n",
    "    acc = sum(y_pred[:,i]*Y_test[:,i])\n",
    "    print('Accuracy for class {}: {}'.format(i,acc/Y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
