{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:QualDR dataset labels unavailable because pyjq not installed\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Boilerplate to implement training different networks on different datasets\n",
    "with varying config.\n",
    "\n",
    "I wish a machine could automate setting up decent baseline models and datasets\n",
    "\"\"\"\n",
    "#  import json\n",
    "import os\n",
    "from os.path import exists\n",
    "import pampy\n",
    "from simple_parsing import ArgumentParser, choice\n",
    "from simplepytorch import datasets as D\n",
    "from simplepytorch import trainlib as TL\n",
    "from simplepytorch import metrics\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Union, Optional\n",
    "import dataclasses as dc\n",
    "import numpy as np\n",
    "import torch as T\n",
    "import torchvision.transforms as tvt\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torchvision import models, transforms\n",
    "\n",
    "from deepfix.models import get_effnetv2, get_resnet, get_efficientnetv1, get_DeepFixEnd2End, DeepFixMLP\n",
    "from deepfix.models.ghaarconv import convert_conv2d_to_gHaarConv2d\n",
    "from deepfix.init_from_distribution import init_from_beta, reset_optimizer\n",
    "from deepfix import deepfix_strategies as dfs\n",
    "import pytorch_wavelets as pyw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "# from ipynb.fs.defs.q2 import *\n",
    "import sklearn\n",
    "import torch\n",
    "import torchvision.datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "# YOUR CODE HERE\n",
    "# Use GPU if available, otherwise stick with cpu\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(123)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"device = {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ln -sr /ocean/projects/cie160013p/agaudio/data/CheXpert-v1.0-small ~/store/DeepFix/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "    ('effnetv2', str, str, str): (\n",
    "        lambda pretrain, in_ch, out_ch: get_effnetv2(pretrain, int(in_ch), int(out_ch))),\n",
    "    ('resnet50', str, str, str): (\n",
    "        lambda pretrain, in_ch, out_ch: get_resnet('resnet50', pretrain, int(in_ch), int(out_ch))),\n",
    "    ('resnet18', str, str, str): (\n",
    "        lambda pretrain, in_ch, out_ch: get_resnet('resnet18', pretrain, int(in_ch), int(out_ch))),\n",
    "    ('efficientnet-b0', str, str, str): (\n",
    "        lambda pretrain, in_ch, out_ch: get_efficientnetv1('efficientnet-b0', pretrain, int(in_ch), int(out_ch))),\n",
    "    ('efficientnet-b1', str, str, str): (\n",
    "        lambda pretrain, in_ch, out_ch: get_efficientnetv1('efficientnet-b1', pretrain, int(in_ch), int(out_ch))),\n",
    "    ('waveletres18', str, str, str): lambda pretrain, in_ch, out_ch: R(\n",
    "        pretrain, int(in_ch), int(out_ch)),\n",
    "    ('waveletmlp', str, str, str, str, str, str, str): (\n",
    "        lambda mlp_channels, in_ch, out_ch, wavelet_levels, patch_size, in_ch_mul, mlp_depth: get_DeepFixEnd2End(\n",
    "            int(in_ch), int(out_ch),\n",
    "            in_ch_multiplier=int(in_ch_mul), wavelet='db1',\n",
    "            wavelet_levels=int(wavelet_levels), wavelet_patch_size=int(patch_size),\n",
    "            mlp_depth=int(mlp_depth), mlp_channels=int(mlp_channels),\n",
    "            mlp_fix_weights='none', mlp_activation=None)\n",
    "        ),\n",
    "\n",
    "    #  ('waveletres18v2', str, str, str): lambda pretrain, in_ch, out_ch: (\n",
    "        #  DeepFixCompression(levels=8, wavelet='coif1', patch_size=1),\n",
    "        #  R2(pretrain, int(in_ch), int(out_ch))),\n",
    "}\n",
    "\n",
    "\n",
    "class R(T.nn.Module):\n",
    "    def __init__(self, pretrain, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.r = get_resnet('resnet18', pretrain, in_ch, out_ch,)\n",
    "        self.dwt = pyw.DWT(J=8, wave='coif1', mode='zero')\n",
    "\n",
    "    @staticmethod\n",
    "    def wavelet_coefficients_as_tensorimage(approx, detail, normalize=False):\n",
    "        B,C = approx.shape[:2]\n",
    "        fixed_dims = approx.shape[:-2] # num images in minibatch, num channels, etc\n",
    "        output_shape = fixed_dims + (\n",
    "            detail[0].shape[-2]*2,  # input img height\n",
    "            detail[0].shape[-1]*2)  # input img width\n",
    "        im = T.zeros(output_shape, device=approx.device, dtype=approx.dtype)\n",
    "        if normalize:\n",
    "            norm11 = lambda x: (x / max(x.min()*-1, x.max()))  # into [-1,+1] preserving sign\n",
    "            #  approx = norm11(approx)\n",
    "        im[..., :approx.shape[-2], :approx.shape[-1]] = approx if approx is not None else 0\n",
    "        for level in detail:\n",
    "            lh, hl, hh = level.unbind(-3)\n",
    "            h,w = lh.shape[-2:]\n",
    "            if normalize:\n",
    "                lh, hl, hh = [norm11(x) for x in [lh, hl, hh]]\n",
    "            #  im[:h, :w] = approx\n",
    "            im[..., 0:h, w:w+w] = lh  # horizontal\n",
    "            im[..., h:h+h, :w] = hl  # vertical\n",
    "            im[..., h:h+h, w:w+w] = hh  # diagonal\n",
    "        return im\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.wavelet_coefficients_as_tensorimage(*self.dwt(x))\n",
    "        return self.r(x)\n",
    "\n",
    "\n",
    "class R2(T.nn.Module):\n",
    "    def __init__(self, pretrain, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.r = get_resnet('resnet18', pretrain, in_ch, out_ch,)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,C,H = x.shape\n",
    "        x = x.unsqueeze(-1).repeat(1,1,1,H)\n",
    "        return self.r(x)\n",
    "\n",
    "\n",
    "class LossCheXpertIdentity(T.nn.Module):\n",
    "    def __init__(self, N):\n",
    "        super().__init__()\n",
    "        self.bce = T.nn.BCEWithLogitsLoss()\n",
    "        self.N = N\n",
    "\n",
    "    def forward(self, yhat, y):\n",
    "        # absolute max possible num patients in chexpert is 223414\n",
    "        # but let's just hash them into a smaller number of bins via modulo N\n",
    "        assert self.N == yhat.shape[1], \\\n",
    "                f'note: model must have {self.N} binary predictions per sample'\n",
    "        y_onehot = y.new_zeros(y.shape[0], self.N, dtype=T.float\n",
    "                               ).scatter_(1, y.long()%self.N, 1)\n",
    "        return self.bce(yhat[:, -1], y_onehot[:, -1])\n",
    "\n",
    "\n",
    "class LossCheXpertUignore(T.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bce = T.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, yhat, y):\n",
    "        ignore = (y != 2)  # ignore uncertainty labels\n",
    "        return self.bce(yhat[ignore], y[ignore])\n",
    "\n",
    "\n",
    "def loss_intelmobileodt(yhat, y):\n",
    "    \"\"\"BCE Loss with class balancing weights.\n",
    "\n",
    "    Not sure this actually helps\n",
    "\n",
    "    because Type 2 is the hardest class, it\n",
    "    has the most samples, and it separates Type 1 from Type 3.  Arguably, Type 2\n",
    "    samples are on the decision boundary between Type 1 and 3.\n",
    "    Class balancing weights make it harder to focus on class 2.\n",
    "    \"\"\"\n",
    "    #  assert y.shape == yhat.shape, 'sanity check'\n",
    "    #  assert y.dtype == yhat.dtype, 'sanity check'\n",
    "\n",
    "    # class distribution of stage='train'\n",
    "    w = T.tensor([249, 781, 450], dtype=y.dtype, device=y.device)\n",
    "    w = (w.max() / w).reshape(1, 3)\n",
    "    # w can have any of the shapes:  (B,1) or (1,C) or (B,C)\n",
    "    #  return T.nn.functional.binary_cross_entropy_with_logits(yhat, y, weight=w)\n",
    "    return T.nn.functional.cross_entropy(yhat, y, weight=w)\n",
    "    # can't apply focal loss unless do it manually.\n",
    "\n",
    "\n",
    "def onehot(y, nclasses):\n",
    "    return T.zeros((y.numel(), nclasses), dtype=y.dtype, device=y.device)\\\n",
    "            .scatter_(1, y.unsqueeze(1), 1)\n",
    "\n",
    "\n",
    "def _upsample_pad_minibatch_imgs_to_same_size(batch, target_is_segmentation_mask=False):\n",
    "    \"\"\"a collate function for a dataloader of (x,y) samples.  \"\"\"\n",
    "    shapes = [item[0].shape for item in batch]\n",
    "    H = max(h for c,h,w in shapes)\n",
    "    W = max(w for c,h,w in shapes)\n",
    "    X, Y = [], []\n",
    "    for item in batch:\n",
    "        h,w = item[0].shape[1:]\n",
    "        dh, dw = (H-h), (W-w)\n",
    "        padding = (dw//2, dw-dw//2, dh//2, dh-dh//2, )\n",
    "        X.append(T.nn.functional.pad(item[0], padding))\n",
    "        if target_is_segmentation_mask:\n",
    "            Y.append(T.nn.functional.pad(item[1], padding))\n",
    "        else:\n",
    "            Y.append(item[1])\n",
    "    return T.stack(X), T.stack(Y)\n",
    "\n",
    "\n",
    "def get_dset_chexpert(train_frac=.8, val_frac=.2, small=False,\n",
    "                      labels:str='diagnostic', num_identities=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        labels:  either \"diagnostic\" (the 14 classes defined as\n",
    "            D.CheXpert.LABELS_DIAGNOSTIC) or \"identity\" (\"patient\", \"study\",\n",
    "            \"view\", \"index\")\n",
    "        small:  whether to use CheXpert_Small dataset (previously downsampled\n",
    "            images) or the fully size dataset.\n",
    "        num_identities:  used only if labels='identity'.  If\n",
    "            num_identities=1000, then all patients get identified as coming\n",
    "            from precisely 1 of 1000 bins.\n",
    "\n",
    "    Returns:\n",
    "        (\n",
    "        {'train_dset': ..., 'val_dset': ..., 'test_dset': ...,\n",
    "         'train_loader': ..., 'val_loader': ..., 'test_loader': ...\n",
    "         },\n",
    "\n",
    "        ('Pneumonia', 'Cardiomegaly', ...)  # class names defined by `labels`\n",
    "        )\n",
    "    \"\"\"\n",
    "    _label_cleanup_dct = dict(D.CheXpert.LABEL_CLEANUP_DICT)\n",
    "    if labels == 'diagnostic':\n",
    "        class_names = D.CheXpert.LABELS_DIAGNOSTIC\n",
    "        for k in class_names:\n",
    "            _label_cleanup_dct[k][np.nan] = 0  # remap missing value to negative\n",
    "        get_ylabels = lambda dct: \\\n",
    "                D.CheXpert.format_labels(dct, labels=class_names).float()\n",
    "    elif labels == 'identity':\n",
    "        class_names = list(range(num_identities))\n",
    "        get_ylabels = lambda dct: \\\n",
    "                (D.CheXpert.format_labels(dct, labels=['index']) % num_identities).long()\n",
    "    else:\n",
    "        raise NotImplementedError(f\"unrecognized labels: {labels}\")\n",
    "    kws = dict(\n",
    "        img_transform=tvt.Compose([\n",
    "            #  tvt.RandomCrop((512, 512)),\n",
    "            tvt.ToTensor(),  # full res 1024x1024 imgs\n",
    "            tvt.Resize((224, 224)),\n",
    "            tvt.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "            \n",
    "        ]),\n",
    "        getitem_transform=lambda dct: (dct['image'], get_ylabels(dct)),\n",
    "        label_cleanup_dct=_label_cleanup_dct,\n",
    "    )\n",
    "    if small:\n",
    "        kls = D.CheXpert_Small\n",
    "    else:\n",
    "        kls = D.CheXpert\n",
    "\n",
    "    train_dset = kls(dataset_dir=\"../data/CheXpert-v1.0-small/\",use_train_set=True, **kws)  #Edited Elvin\n",
    "\n",
    "    N = len(train_dset)\n",
    "    if train_frac + val_frac == 1:\n",
    "        nsplits = [N - int(N*val_frac), int(N*val_frac), 0]\n",
    "    else:\n",
    "        a,b = int(N*train_frac), int(N*val_frac)\n",
    "        nsplits = [a,b, N-a-b]\n",
    "    train_dset, val_dset, _ = T.utils.data.random_split(train_dset, nsplits)\n",
    "    test_dset = kls(dataset_dir=\"../data/CheXpert-v1.0-small/\",use_train_set=False, **kws) #Edited Elvin\n",
    "    batch_dct = dict(\n",
    "        batch_size=1, collate_fn=_upsample_pad_minibatch_imgs_to_same_size,\n",
    "        num_workers=int(os.environ.get(\"num_workers\", 4)))  # upsample pad must take time\n",
    "    train_loader=DataLoader(train_dset, shuffle=True, **batch_dct)\n",
    "    val_loader=DataLoader(val_dset, **batch_dct)\n",
    "    test_loader=DataLoader(test_dset, **batch_dct)\n",
    "    return (dict(\n",
    "        train_dset=train_dset, val_dset=val_dset, test_dset=test_dset,\n",
    "        train_loader=train_loader, val_loader=val_loader, test_loader=test_loader,\n",
    "    ), class_names)\n",
    "\n",
    "\n",
    "\n",
    "def match(spec:str, dct:dict):\n",
    "    return pampy.match(spec.split(':'), *(x for y in dct.items() for x in y))\n",
    "\n",
    "\n",
    "def get_model_opt_loss(\n",
    "        model_spec:str, opt_spec:str, loss_spec:str, regularizer_spec:str,\n",
    "        device:str) -> dict[str, Union[T.nn.Module, T.optim.Optimizer]]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model_spec: a string of form,\n",
    "            \"model_name:pretraining:in_channels:out_classes\".  For example:\n",
    "            \"effnetv2:untrained:1:5\"\n",
    "        opt_spec: Specifies how to create optimizer.\n",
    "            First value is a pytorch Optimizer in T.optim.*.\n",
    "            Other values are numerical parameters.\n",
    "            Example: \"SGD:lr=.003:momentum=.9\"\n",
    "        device: e.g. 'cpu' or 'gpu'\n",
    "    Returns:\n",
    "        a pytorch model and optimizer\n",
    "    \"\"\"\n",
    "    mdl = match(model_spec, MODELS)\n",
    "    mdl = mdl.to(device, non_blocking=True)\n",
    "    optimizer = reset_optimizer(opt_spec, mdl)\n",
    "    loss_fn = match(loss_spec, LOSS_FNS)\n",
    "    if regularizer_spec != 'none':\n",
    "        loss_fn = RegularizedLoss(mdl, loss_fn, regularizer_spec)\n",
    "    return dict(model=mdl, optimizer=optimizer, loss_fn=loss_fn)\n",
    "\n",
    "\n",
    "class RegularizedLoss(T.nn.Module):\n",
    "    def __init__(self, model, lossfn, regularizer_spec:str):\n",
    "        super().__init__()\n",
    "        self.lossfn = lossfn\n",
    "        self.regularizer_spec = regularizer_spec\n",
    "        if regularizer_spec == 'none':\n",
    "            self.regularizer = lambda *y: 0\n",
    "        elif regularizer_spec.startswith('deepfixmlp:'):\n",
    "            lbda = float(regularizer_spec.split(':')[1])\n",
    "            self.regularizer = lambda *y: (\n",
    "                float(lbda) * DeepFixMLP.get_VecAttn_regularizer(model))\n",
    "        else:\n",
    "            raise NotImplementedError(regularizer_spec)\n",
    "\n",
    "    def forward(self, yhat, y):\n",
    "        return self.lossfn(yhat, y) + self.regularizer(yhat, y)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'RegularizedLoss<{repr(self.lossfn)},{self.regularizer_spec}>'\n",
    "\n",
    "\n",
    "def get_dset_loaders_resultfactory(dset_spec:str) -> dict:\n",
    "    dct, class_names = match(dset_spec, DSETS)\n",
    "    if any(dset_spec.startswith(x) for x in {'intel_mobileodt:',\n",
    "                                             'chexpert_small_ID:'}):\n",
    "        #  dct['result_factory'] = lambda: TL.MultiLabelBinaryClassification(\n",
    "                #  class_names, binarize_fn=lambda yh: (T.sigmoid(yh)>.5).long())\n",
    "        dct['result_factory'] = lambda: TL.MultiClassClassification(\n",
    "                len(class_names), binarize_fn=lambda yh: yh.softmax(1).argmax(1))\n",
    "    elif any(dset_spec.startswith(x) for x in {'chexpert:', 'chexpert_small:'}):\n",
    "        dct['result_factory'] = lambda: CheXpertMultiLabelBinaryClassification(\n",
    "            class_names, binarize_fn=lambda yh: (yh.sigmoid()>.5).long(), report_avg=True)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"I don't know how to create the result factory for {dset_spec}\")\n",
    "    return dct\n",
    "\n",
    "class CheXpertMultiLabelBinaryClassification(TL.MultiLabelBinaryClassification):\n",
    "    def update(self, yhat, y, loss) -> None:\n",
    "        self.num_samples += yhat.shape[0]\n",
    "        self.loss += loss.item()\n",
    "        assert yhat.shape == y.shape\n",
    "        assert yhat.ndim == 2 and yhat.shape[1] == len(self._cms), \"sanity check: model outputs expected prediction shape\"\n",
    "        binarized = self._binarize_fn(yhat)\n",
    "        assert binarized.dtype == T.long, 'sanity check binarize fn'\n",
    "        assert binarized.shape == y.shape, 'sanity check binarize fn'\n",
    "        ignore = (y != 2)  # ignore uncertainty labels\n",
    "        for i, (kls, cm) in enumerate(self._cms.items()):\n",
    "            rows = ignore[:, i]\n",
    "            if rows.sum() == 0:\n",
    "                continue  # don't update a confusion matrix if all data for this class is ignored\n",
    "            cm += metrics.confusion_matrix(y[rows, i], binarized[rows, i], num_classes=2).cpu()\n",
    "\n",
    "\n",
    "def get_deepfix_train_strategy(args:'TrainOptions'):\n",
    "    deepfix_spec = args.deepfix\n",
    "    if deepfix_spec == 'off':\n",
    "        return TL.train_one_epoch\n",
    "    elif deepfix_spec.startswith('reinit:'):\n",
    "        _, N, P, R = deepfix_spec.split(':')\n",
    "        return dfs.DeepFix_TrainOneEpoch(int(N), float(P), int(R), TL.train_one_epoch)\n",
    "    elif deepfix_spec.startswith('dhist:'):\n",
    "        fp = deepfix_spec.split(':', 1)[1]\n",
    "        assert exists(fp), f'histogram file not found: {fp}'\n",
    "        return dfs.DeepFix_DHist(fp)\n",
    "    elif deepfix_spec.startswith('dfhist:'):\n",
    "        fp = deepfix_spec.split(':', 1)[1]\n",
    "        assert exists(fp), f'histogram file not found: {fp}'\n",
    "        return dfs.DeepFix_DHist(fp, fixed=True)\n",
    "    elif deepfix_spec == 'fixed':\n",
    "        return dfs.DeepFix_DHist('', fixed=True, init_with_hist=False)\n",
    "    elif deepfix_spec.startswith('beta:'):\n",
    "        alpha, beta = deepfix_spec.split(':')[1:]\n",
    "        return dfs.DeepFix_LambdaInit(\n",
    "            lambda cfg: init_from_beta(cfg.model, float(alpha), float(beta)))\n",
    "    elif deepfix_spec.startswith('ghaarconv2d:'):\n",
    "        ignore_layers = deepfix_spec.split(':')[1].split(',')\n",
    "        return dfs.DeepFix_LambdaInit(\n",
    "            lambda cfg: (\n",
    "                print(f'initialize {deepfix_spec}'),\n",
    "                convert_conv2d_to_gHaarConv2d(cfg.model, ignore_layers=ignore_layers),\n",
    "                reset_optimizer(args.opt, cfg.model),\n",
    "                print(cfg.model)\n",
    "            ))\n",
    "    else:\n",
    "        raise NotImplementedError(deepfix_spec)\n",
    "\n",
    "\n",
    "def train_config(args:'TrainOptions') -> TL.TrainConfig:\n",
    "    return TL.TrainConfig(\n",
    "        **get_model_opt_loss(\n",
    "            args.model, args.opt, args.lossfn, args.loss_reg, args.device),\n",
    "        **get_dset_loaders_resultfactory(args.dset),\n",
    "        device=args.device,\n",
    "        epochs=args.epochs,\n",
    "        start_epoch=args.start_epoch,\n",
    "        train_one_epoch=get_deepfix_train_strategy(args),\n",
    "        experiment_id=args.experiment_id,\n",
    "    )\n",
    "\n",
    "\n",
    "@dc.dataclass\n",
    "class TrainOptions:\n",
    "    \"\"\"High-level configuration for training PyTorch models\n",
    "    on the IntelMobileODTCervical dataset.\n",
    "    \"\"\"\n",
    "    epochs:int = 50\n",
    "    start_epoch:int = 0  # if \"--start_epoch 1\", then don't evaluate perf before training.\n",
    "    device:str = 'cuda' if T.cuda.is_available() else 'cpu'\n",
    "    dset:str = None #choice(\n",
    "        #  'intel_mobileodt:train:val:test:v1',\n",
    "        #  'intel_mobileodt:train+additional:val:test:v1',\n",
    "        #  'intel_mobileodt:train+additional:noval:test:v1',\n",
    "        #  'chexpert:.8:.2', 'chexpert:.01:.01', 'chexpert:.001:.001',\n",
    "        #  'chexpert_small:.8:.2', 'chexpert_small:.01:.01',\n",
    "        #   'chexpert_small:.001:.001',\n",
    "        #  default='intel_mobileodt:train:val:test:v1')\n",
    "    opt:str = 'SGD:lr=.001:momentum=.9:nesterov=1'\n",
    "    lossfn:str = None  # choices:\n",
    "        #  'BCEWithLogitsLoss',\n",
    "        #  'CrossEntropyLoss', \n",
    "        #  'CE_intelmobileodt',\n",
    "        #  'chexpert_uignore', \n",
    "        #  'chexpert_identity:N' for some N=num_identities predicted by model (compared to identities y%N)\n",
    "    loss_reg:str = 'none'  # Optionally add a regularizer to the loss.  loss + reg.  Accepted values:  'none', 'deepfixmlp:X' where X is a positive float denoting the lambda in l1 regularizer\n",
    "    model:str = 'resnet18:imagenet:3:3'  # Model specification adheres to the template \"model_name:pretraining:in_ch:out_ch\"\n",
    "    deepfix:str = 'off'  # DeepFix Re-initialization Method.\n",
    "                         #  \"off\" or \"reinit:N:P:R\" or \"d[f]hist:path_to_histogram.pth\"\n",
    "                         #  or \"beta:A:B\" for A,B as (float) parameters of the beta distribution\n",
    "                         # 'ghaarconv2d:layer1,layer2' Replaces all spatial convolutions with GHaarConv2d layer except the specified layers\n",
    "    experiment_id:str = os.environ.get('run_id', 'debugging')\n",
    "    prune:str = 'off'\n",
    "\n",
    "    def execute(self):\n",
    "        cfg = train_config(self)\n",
    "        cfg.train(cfg)\n",
    "\n",
    "\n",
    "def main():\n",
    "    p = ArgumentParser()\n",
    "    p.add_arguments(TrainOptions, dest='TrainOptions')\n",
    "#     for patch_size in [1,32]:\n",
    "#         for wavelet_level in [1,2,3,4,5,6,7,8,9]:    \n",
    "#             try:\n",
    "    in_ch, out_ch = 3, 3\n",
    "    model_params = \"resnet18:imagenet:\"+str(in_ch)+\":\"+str(in_ch)    \n",
    "    \n",
    "#     model_params = \"waveletmlp:300:1:14:\"+str(patch_size)+\":\"+str(wavelet_level)+\":1:2\"\n",
    "    exp_id = 'model_'+model_params+'_in_ch_'+str(in_ch)+'out_ch_'+str(in_ch)#+'_patch_size_' + str(patch_size) + '_level_' + str(wavelet_level)\n",
    "    args = p.parse_args([\"--dset\", \"chexpert_small:.01:.01\", \"--opt\", \"Adam:lr=0.001\", \"--lossfn\", \"chexpert_uignore\", \"--model\", model_params, \"--loss_reg\", \"none\",\"--experiment_id\",exp_id]).TrainOptions\n",
    "\n",
    "    print(args)\n",
    "    cfg = train_config(args)\n",
    "\n",
    "# python deepfix/train.py --dset chexpert_small:.01:.01 --opt Adam:lr=0.001 --lossfn chexpert_uignore --model waveletmlp:300:1:14:7:1:1:2 --loss_reg none    \n",
    "\n",
    "    if args.prune != 'off':\n",
    "        assert args.prune.startswith('ChannelPrune:')\n",
    "        raise NotImplementedError('code is a bit hardcoded, so it is not available without hacking on it.')\n",
    "        print(args.prune)\n",
    "        from explainfix import channelprune\n",
    "        from deepfix.weight_saliency import costfn_multiclass\n",
    "        a = sum([x.numel() for x in cfg.model.parameters()])\n",
    "        channelprune(cfg.model, pct=5, grad_cost_fn=costfn_multiclass,\n",
    "                     loader=cfg.train_loader, device=cfg.device, num_minibatches=10)\n",
    "        b = sum([x.numel() for x in cfg.model.parameters()])\n",
    "        assert a/b != 1\n",
    "        print(f'done channelpruning.  {a/b}')\n",
    "\n",
    "    cfg.train(cfg)\n",
    "#             except Exception as e:\n",
    "#                 print(\"=================================================================================================\")\n",
    "#                 print(e)\n",
    "#                 print(\"=================================================================================================\")\n",
    "            \n",
    "    print('+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "    #  import IPython ; IPython.embed() ; import sys ; sys.exit()\n",
    "\n",
    "    #  with T.profiler.profile(\n",
    "    #      activities=[\n",
    "    #          T.profiler.ProfilerActivity.CPU,\n",
    "    #          T.profiler.ProfilerActivity.CUDA,\n",
    "    #      ], with_modules=True,\n",
    "    #  ) as p:\n",
    "    #      cfg.train(cfg)\n",
    "    #  print(p.key_averages().table(\n",
    "    #      sort_by=\"self_cuda_time_total\", row_limit=-1))\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_dset': <torch.utils.data.dataset.Subset at 0x1468202f4c70>,\n",
       " 'val_dset': <torch.utils.data.dataset.Subset at 0x1468202f4e50>,\n",
       " 'test_dset': <simplepytorch.datasets.chexpert.CheXpert at 0x146826e4dbe0>,\n",
       " 'train_loader': <torch.utils.data.dataloader.DataLoader at 0x14682771c850>,\n",
       " 'val_loader': <torch.utils.data.dataloader.DataLoader at 0x1468202920d0>,\n",
       " 'test_loader': <torch.utils.data.dataloader.DataLoader at 0x146820292cd0>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d,l = get_dset_chexpert()\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainset_loader = d['train_dset']\n",
    "# valset_loader = d['val_dset']\n",
    "\n",
    "trainset_loader = d['train_loader']\n",
    "valset_loader = d['val_loader']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178732"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "new_model = new_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 178732/178732 [29:07<00:00, 102.26it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Will contain the feature\n",
    "X_train = []\n",
    "Y_train = []\n",
    "for data in tqdm(trainset_loader):\n",
    "    inputs, labels = data\n",
    "    inputs = inputs.to(device)\n",
    "    labels = torch.max(labels, 1)[1].to(device)\n",
    "    with torch.no_grad():\n",
    "        # Extract the feature from the image\n",
    "        feature = new_model(inputs)\n",
    "#         print(feature.size())\n",
    "        # Convert to NumPy Array, Reshape it, and save it to features variable\n",
    "        X_train.append(feature.cpu().detach().numpy().reshape(512))\n",
    "        Y_train.append(labels.cpu().detach().numpy()[0])\n",
    "#         features.append(feature.cpu().detach().numpy().reshape(-1,4096))\n",
    "# Convert to NumPy Array\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44682/44682 [07:28<00:00, 99.65it/s] \n"
     ]
    }
   ],
   "source": [
    "# Will contain the feature\n",
    "X_test = []\n",
    "Y_test = []\n",
    "for data in tqdm(valset_loader):\n",
    "    inputs, labels = data\n",
    "    inputs = inputs.to(device)\n",
    "    labels = torch.max(labels, 1)[1].to(device)\n",
    "    with torch.no_grad():\n",
    "        # Extract the feature from the image\n",
    "        feature = new_model(inputs)\n",
    "#         print(feature.size())\n",
    "        # Convert to NumPy Array, Reshape it, and save it to features variable\n",
    "        X_test.append(feature.cpu().detach().numpy().reshape(512))\n",
    "        Y_test.append(labels.cpu().detach().numpy()[0])\n",
    "#         features.append(feature.cpu().detach().numpy().reshape(-1,4096))\n",
    "# Convert to NumPy Array\n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train).to_csv(\"feature_csv/resnet18_x_train.csv\", index=False)  \n",
    "pd.DataFrame(Y_train).to_csv(\"feature_csv/resnet18_y_train.csv\", index=False)\n",
    "# X_train = pd.read_csv(\"feature_csv/resnet18_x_train.csv\")\n",
    "# Y_train = pd.read_csv(\"feature_csv/resnet18_y_train.csv\")\n",
    "\n",
    "pd.DataFrame(X_test).to_csv(\"feature_csv/resnet18_x_test.csv\", index=False)  \n",
    "pd.DataFrame(Y_test).to_csv(\"feature_csv/resnet18_y_test.csv\", index=False)\n",
    "# X_test = pd.read_csv(\"feature_csv/resnet18_x_test.csv\")\n",
    "# Y_test = pd.read_csv(\"feature_csv/resnet18_y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(X_train).to_csv(\"feature_csv/vgg16_x_train.csv\", index=False)  \n",
    "# pd.DataFrame(Y_train).to_csv(\"feature_csv/vgg16_y_train.csv\", index=False)\n",
    "X_train = pd.read_csv(\"feature_csv/resnet18_x_train.csv\")\n",
    "Y_train = pd.read_csv(\"feature_csv/resnet18_y_train.csv\")\n",
    "\n",
    "# pd.DataFrame(X_test).to_csv(\"feature_csv/vgg16_x_test.csv\", index=False)  \n",
    "# pd.DataFrame(Y_test).to_csv(\"feature_csv/vgg16_y_test.csv\", index=False)\n",
    "X_test = pd.read_csv(\"feature_csv/resnet18_x_test.csv\")\n",
    "Y_test = pd.read_csv(\"feature_csv/resnet18_y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalised = pd.DataFrame(normalize(X_train, axis = 0))\n",
    "# std_scale = sklearn.preprocessing.StandardScaler().fit(X_train)\n",
    "# X_train = std_scale.transform(X_train)\n",
    "# X_test  = std_scale.transform(X_test)\n",
    "# 2. Finding covariance matrix\n",
    "# X_train = pd.DataFrame(X_train)\n",
    "# X_test  = pd.DataFrame(X_test)\n",
    "\n",
    "# covariance_df = X_train.cov()\n",
    "# # 3. Eigen Vectors\n",
    "# # print(normalised.shape)\n",
    "# u, s, v = np.linalg.svd(covariance_df)\n",
    "# # 4. Principal Components\n",
    "# # data_reduced = normalised @ u[:620]\n",
    "# # print(u.shape)\n",
    "# X_train = pd.DataFrame(X_train@u[:,:1024])\n",
    "# X_test =  pd.DataFrame(X_test@u[:,:1024])\n",
    "\n",
    "# data_reduced.head()\n",
    "\n",
    "# data_reduced.to_csv(\"pc\" + WAVELET + str(MAX_LEVEL) + \".csv\", index=False)\n",
    "# pcs = pd.read_csv(\"pc\" + WAVELET + str(MAX_LEVEL)+ \".csv\")\n",
    "# outputs = pd.read_csv(\"outputs_main.csv\", header=None)\n",
    "# X = pcs.iloc[:, :].values\n",
    "# Y = outputs.iloc[:, :].value\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GMM Train               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.09      0.13     21175\n",
      "           1       0.09      0.12      0.10     15540\n",
      "           2       0.11      0.07      0.09     19297\n",
      "           3       0.26      0.09      0.14     38828\n",
      "           4       0.01      0.03      0.02      2928\n",
      "           5       0.05      0.02      0.03     19503\n",
      "           6       0.12      0.06      0.08     20850\n",
      "           7       0.05      0.09      0.07      9721\n",
      "           8       0.07      0.11      0.08     10807\n",
      "           9       0.04      0.10      0.06      6253\n",
      "          10       0.05      0.03      0.04      9295\n",
      "          11       0.00      0.06      0.01       638\n",
      "          12       0.01      0.06      0.01      1906\n",
      "          13       0.01      0.08      0.02      1991\n",
      "\n",
      "    accuracy                           0.08    178732\n",
      "   macro avg       0.08      0.07      0.06    178732\n",
      "weighted avg       0.14      0.08      0.09    178732\n",
      "\n",
      "GMM Test               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.09      0.14      5352\n",
      "           1       0.09      0.12      0.10      3969\n",
      "           2       0.11      0.08      0.09      4870\n",
      "           3       0.25      0.09      0.13      9611\n",
      "           4       0.02      0.03      0.02       745\n",
      "           5       0.04      0.02      0.03      4824\n",
      "           6       0.12      0.05      0.07      5242\n",
      "           7       0.05      0.08      0.06      2456\n",
      "           8       0.07      0.11      0.08      2697\n",
      "           9       0.04      0.09      0.06      1507\n",
      "          10       0.05      0.03      0.04      2300\n",
      "          11       0.00      0.03      0.00       157\n",
      "          12       0.01      0.06      0.01       486\n",
      "          13       0.01      0.08      0.02       466\n",
      "\n",
      "    accuracy                           0.08     44682\n",
      "   macro avg       0.08      0.07      0.06     44682\n",
      "weighted avg       0.14      0.08      0.09     44682\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gmm=GaussianMixture(n_components=14,init_params='random',covariance_type='spherical',tol=0.001).fit(X_train)\n",
    "y_pred_train=gmm.predict(X_train)\n",
    "y_pred_test=gmm.predict(X_test)\n",
    "\n",
    "\n",
    "report_test = classification_report(Y_test, y_pred_test)\n",
    "report_train = classification_report(Y_train, y_pred_train)\n",
    "\n",
    "print('GMM Train',report_train)\n",
    "print('GMM Test',report_test)\n",
    "\n",
    "# svc = SVC(kernel='linear')\n",
    "# svc.fit(X_train, Y_train)\n",
    "# accuracy = svc.score(X_test, Y_test)\n",
    "# print(\"Accuracy on the training set is: {0:.1f}%\".format(accuracy_train*100))\n",
    "# print(\"Accuracy on the testing set is: {0:.1f}%\".format(accuracy*100))\n",
    "# prediction = svc.predict(X_test)\n",
    "# report = classification_report(Y_test, prediction)\n",
    "# print('SVM',report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc = SVC()\n",
    "# parameters = {\"C\": (100, 1e3, 1e4, 1e5, 1e6, 1e7, 1e8, 1e9), \"gamma\": (1e-08, 1e-7, 1e-6, 1e-5),\"kernel\":(\"linear\", \"rbf\",\"poly\")}\n",
    "# grid_search = GridSearchCV(svc, parameters, n_jobs=-1, cv=5)\n",
    "# start_time = timeit.default_timer()\n",
    "# grid_search.fit(X_train, Y_train)\n",
    "# #     print(\"--- {0:.3f} seconds ---\".format(timeit.default_timer() - start_time))\n",
    "# print(grid_search.best_params_)\n",
    "# svc_best = grid_search.best_estimator_\n",
    "# accuracy = svc_best.score(X_test, Y_test)\n",
    "# accuracy_train = svc_best.score(X_train, Y_train)\n",
    "# print(\"Accuracy on the training set is: {0:.1f}%\".format(accuracy_train*100))\n",
    "# print(\"Accuracy on the testing set is: {0:.1f}%\".format(accuracy*100))\n",
    "# prediction = svc_best.predict(X_test)\n",
    "# report = classification_report(Y_test, prediction)\n",
    "# print('RESULTS FOR WAVELET: ',WAVELET)\n",
    "# print('SVM',report)\n",
    "\n",
    "# svc = SVC(C=10000,gamma=1e-08,kernel='poly')\n",
    "# svc.fit(X_train, Y_train)\n",
    "# accuracy = svc.score(X_test, Y_test)\n",
    "# print(\"Accuracy on the training set is: {0:.1f}%\".format(accuracy_train*100))\n",
    "# print(\"Accuracy on the testing set is: {0:.1f}%\".format(accuracy*100))\n",
    "# prediction = svc.predict(X_test)\n",
    "# report = classification_report(Y_test, prediction)\n",
    "# print('SVM',report)\n",
    "\n",
    "svc = SVC(kernel='linear')\n",
    "svc.fit(X_train, Y_train)\n",
    "accuracy = svc.score(X_test, Y_test)\n",
    "print(\"Accuracy on the training set is: {0:.1f}%\".format(accuracy_train*100))\n",
    "print(\"Accuracy on the testing set is: {0:.1f}%\".format(accuracy*100))\n",
    "prediction = svc.predict(X_test)\n",
    "report = classification_report(Y_test, prediction)\n",
    "print('SVM',report)\n",
    "\n",
    "svc = SVC(kernel='rbf')\n",
    "svc.fit(X_train, Y_train)\n",
    "accuracy = svc.score(X_test, Y_test)\n",
    "print(\"Accuracy on the training set is: {0:.1f}%\".format(accuracy_train*100))\n",
    "print(\"Accuracy on the testing set is: {0:.1f}%\".format(accuracy*100))\n",
    "prediction = svc.predict(X_test)\n",
    "report = classification_report(Y_test, prediction)\n",
    "print('SVM',report)\n",
    "\n",
    "\n",
    "# svc = SVC(C=10000,gamma=1e-08,kernel='linear')\n",
    "# svc.fit(X_train, Y_train)\n",
    "# accuracy = svc.score(X_test, Y_test)\n",
    "# print(\"Accuracy on the training set is: {0:.1f}%\".format(accuracy_train*100))\n",
    "# print(\"Accuracy on the testing set is: {0:.1f}%\".format(accuracy*100))\n",
    "# prediction = svc.predict(X_test)\n",
    "# report = classification_report(Y_test, prediction)\n",
    "# print('SVM',report)\n",
    "\n",
    "# svc = SVC(C=10000,gamma=1e-08,kernel='rbf')\n",
    "# svc.fit(X_train, Y_train)\n",
    "# accuracy = svc.score(X_test, Y_test)\n",
    "# print(\"Accuracy on the training set is: {0:.1f}%\".format(accuracy_train*100))\n",
    "# print(\"Accuracy on the testing set is: {0:.1f}%\".format(accuracy*100))\n",
    "# prediction = svc.predict(X_test)\n",
    "# report = classification_report(Y_test, prediction)\n",
    "# print('SVM',report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
